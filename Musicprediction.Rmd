---
title: "Predicting Music Genres"
author: "Samuel Kuenti"
date: "12 5 2022"
output: pdf_document
---

```{r setup, include=FALSE}


# setup the R session, load libraries
knitr::opts_chunk$set(echo = FALSE)
options(digits = 5)

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(randomForest)
library(data.table)
library(dplyr)
library(knitr)
library(ggplot2)

```

## Abstract

*The aim of this project was to predict the music genre of a song based on a number of features of the song. In order to achieve this, a knn-model was compared to a random forest approach. It was expected that while the random forest model would perform reasonably well, the knn-model would be overwhelmed with the number of predictors used. This was confirmed by testing and comparing the models on actual data. Both models could not be optimised fully due to limitations imposed by long computation times.*

\newpage

## Introduction

The aim of this project is to develop a machine learning algorithm to predict the genre of a song based on a number of attributes. Such an algorithm could support the automatic organisation of a collection of digital music.

The project guidelines ask for at least two different modeling approaches to be compared side by side. As will be discussed below, the data in this case provides multiple predictors. Based on theory, simpler modelling approaches like k-nearest neighbors should suffer when too many predictors are involved. In order to explore this theoretical caveat, a k-nearest-neighbor model will be compared to a random forest approach, which should deal better with a larger number of inputs.

## Analysis

### Data Preparation

#### Obtaining Data

This project is based on [data](https://www.kaggle.com/datasets/vicsuperman/prediction-of-music-genre?resource=download) compiled into a CSV file by Gaoyuan. The dataset is available from the public kaggle data repository. Due to issues with implementing kaggle's authentication process in R, the data file was copied to this project's GitHub repository and is loaded from there.

```{r}
Sys.time() # when processing started
# load data from the file in the repo (you should be able to do this after pulling)
music_raw_data <- read.csv("music_genre.csv", header=TRUE)
```

### Initial data inspection

The following table shows what kind of information the dataset contains.

```{r}
# find out what kind of data there are
kable(as.list(colnames(music_raw_data)), caption="Variables in the dataset")
```




#### Data cleaning

An initial look at the raw data reveals that some songs seem to have incomplete information (e.g. tempo = ?, or a duration of -1ms).

```{r}
# initial look at the data
kable(head(music_raw_data))
kable(str(music_raw_data))
```

Also, some variables have unsuitable data types (e.g. tempo is <chr>). This will be corrected. In addition, negative durations of songs will be replaced with NAs (these will be suppressed later).

```{r}
# correcting data format in the raw data
music_raw_data$key <- as.factor(music_raw_data$key)
music_raw_data$mode <- as.factor(music_raw_data$mode)
music_raw_data$tempo <- as.numeric(music_raw_data$tempo)
music_raw_data$music_genre <- as.factor(music_raw_data$music_genre)
music_raw_data$artist_name <- as.factor(music_raw_data$artist_name)

# replace negative durations with NAs
music_raw_data$duration_ms <- na_if(music_raw_data$duration_ms, -1)
# head(music_raw_data)
```

As noted above, there were some problematic table entries, which lead to the generation of NAs.

```{r}
kable(sapply(music_raw_data, {function(x) any(is.na(x))}), caption = "Columns with NAs")
```

There are different options for dealing with NA rows. Here, for simplicity's sake and because the dataset is reasonably large, the lamest option will be implemented - all songs with NAs will be removed from the dataset.

All in all, there are data on `r nrow(music_raw_data)` songs in the dataset (including those with NAs). Those with NAs will be deleted.

```{r}
# nrow(music_raw_data)
music_raw_data <- na.omit(music_raw_data) 
# nrow(music_raw_data)
```

This leads to the loss of about 20% of the data and leaves complete information about `r nrow(music_raw_data)` songs, more than enough as will be seen later on.

```{r}
kable(sapply(music_raw_data, {function(x) any(is.na(x))}), caption = "Columns with NAs after data cleaning")
```

Now the data looks as follows:

```{r}
str(music_raw_data)
```


### Light mode

```{r}
n_fold <- 5 # setting a global n for cross validations
percent <- 50 # only using this many percent of the entire dataset
```

In order to limit the processing times, only `r percent`% of the actual data are used. 

(Note: with 5-fold crossvalidation and 50% of the data, the script took about half an hour to compute on a 2017 iMac.)

Also, crossvalidation will be n-fold, with n being set at `r n_fold` initially. Later on this may be increased to 10, based on processing speeds.

```{r}
# create a subsample of the entire dataset to work with (to control processing speeds)
set.seed(1, sample.kind="Rounding")
light_index <- createDataPartition(music_raw_data$music_genre, times = 1, p = percent/100, list = FALSE)
music_raw_data <- music_raw_data[light_index,]

```

Caution: even with these limits, the whole script will take a long time to run through (it took about 20 minutes on a 2017 iMac).

### Splitting of data

The data will be split into a training set (80% of the data), and a test set (20%). The training set will be used to tune the models. The test set will simulate actual new data in order to assess the final performance of the various models.

```{r}
# create a training (80%) and a test (20%)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(music_raw_data$music_genre, times = 1, p = 0.2, list=FALSE)
# 20% will go into the validation set
test_set <- music_raw_data[test_index,]
test_set <- droplevels(test_set)
# the rest into the train set
train_set <- music_raw_data[-test_index,]
train_set <- droplevels(train_set)

```

## Modelling

The idea is to predict the genre of a song (music_genre). The genre variable distinguishes the following musical genres:

```{r}
kable(unique(music_raw_data$music_genre), caption = "Musical Genres")
length(unique(music_raw_data$music_genre))
```


Some of the parameters in the dataset seem pretty quantifiable, for instance based on the envelope of a song (e.g. key, tempo, popularity), while others are subjective and would have to be rated by listeners (e.g. danceability, energy, valence). A viable initial approach could be to only include objectively quantifiable predictors, in order to produce a lean and sleek algorithm. Subjective parameters based on listeners' ratings would only be included if initial results prove to be too inaccurate.

The following variables are assumed to be based on listeners' subjective rating (there is no information on how the dataset was obtained):

-   acousticness
-   danceability (maybe correlated with tempo?)
-   energy
-   instrumentalness
-   liveness
-   speechiness
-   valence


This leaves the following basic predictors, which are assumed to be objectively measurable by suitable algorithms, without involving actual listeners (with popularity being a bit of a hybrid, it will be included here):

-   popularity
-   key
-   loudness
-   mode
-   tempo
-   duration

The artist name would be a subjective parameter with high predictive power (many artists tend to be in some genre for most of their career). However, being a factor with several thousand levels, this feature was found to increase computation times beyond practical limits, so it is excluded from the models.

The parameters above look fairly generic, and it is doubtful that a song's genre could be reliably predicted based on these parameters alone. Nevertheless, this will be attempted in a first step, and 'subjective parameters' (those based on listeners' ratings) will be included later to see whether they contribute to model performance.

The following variables are deemed irrelevant for the current task and will not be included in the models:

-   ID-number of the song (instance_id)
-   track name (track_name)
-   obtained date (obtained_date)

The models will predict the song's genre (music_genre).

### Models with six objective parameters

As described above, a first attempt will only involve parameters which can potentially be derived from the music by means of appropriate algorithms (e.g. based on the envelope curve). It will be interesting to compare the performance of the two models with a different number of predictors.

As discussed above, the following predictors are used as 'objective parameters':

-   popularity
-   key
-   loudness
-   mode
-   tempo
-   duration

#### knn (six predictors)

A knn-model will be fitted using the six objective predictors above. In order to somewhat limit computation times, `r n_fold`-fold cross-validation will be used to speed up the model fitting, and initial resolution will be low.

```{r}
# train a knn-model with objective parameters
set.seed(1, sample.kind="Rounding")
train_control <- trainControl(method = "cv", number = n_fold, p = .8)
knn_objective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms, # + artist_name, 
                       method = "knn", 
                       data = train_set, 
                       tuneGrid = data.frame(k = seq(2,502,50)),
                       trControl = train_control)
ggplot(knn_objective, highlight = TRUE)
# knn_objective$bestTune$k
```

Note that while the model's accuracy seems to increase with larger ks, at k = 502, the training function reports 'too many ties in knn'. As more neighbors are included into the voting process, the likelihood of ties occurring seems to increase, and in this case, the limit of viable ks seems to be around 500.

In a second step, k-values approaching this limit at k=502 (the first observed k raising an error above) will be examined with better resolution.


```{r}
# train a knn-model with objective parameters
set.seed(1, sample.kind="Rounding")
train_control <- trainControl(method = "cv", number = n_fold, p = .8)
knn_objective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms, # + artist_name, 
                       method = "knn", 
                       data = train_set, 
                       tuneGrid = data.frame(k = seq(460,502,2)),
                       trControl = train_control)
ggplot(knn_objective, highlight = TRUE)
knn_objective$bestTune$k
```



```{r}
knn_objective_accuracy <- confusionMatrix(predict(knn_objective, test_set, type = "raw"), test_set$music_genre)$overall["Accuracy"]
model_results <- tibble(Method = "knn (6 predictors)", Accuracy = knn_objective_accuracy) 
# kable(model_results, caption = "Model accuracies")
```

#### Random forest (seven predictors)

For comparison, a random forest model will be trained on the train_set.

```{r}
# setup of random forest parameters
nodesize_setting <- 5 # treesize, higher number = smaller trees (less branches)
ntree_setting <- 100 # size of the 'forest'
grid <- data.frame(mtry = seq(1,100,5)) # range of mtry = # of variables sampled at splits
```

Fitting random forests weighs heavy on the CPU, so the minimum nodesize is set to `r nodesize_setting` (instead of the default of 1) in order to limit computation time somewhat.

```{r}

set.seed(1, sample.kind="Rounding")
control <- trainControl(method = "cv", number = n_fold)
rf_objective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms, # + artist_name, 
                      method = "rf",
                      ntree = ntree_setting,
                      trControl = control,
                      tuneGrid = grid,
                      nodesize = nodesize_setting,
                      data = train_set)
ggplot(rf_objective)
rf_objective$bestTune$mtry
```
This model did best when `r rf_objective$bestTune$mtry` variables were selected for each tree, with accuracy declining slightly as more were selected.

```{r}
rf_objective_accuracy <- confusionMatrix(predict(rf_objective, test_set, type = "raw"), test_set$music_genre)$overall["Accuracy"]
model_results <- bind_rows(model_results,
                          tibble(Method="Random forest (6 predictors)",
                                     Accuracy = rf_objective_accuracy))
# kable(model_results, caption = "Model accuracies")
```

### Models with six objective in addition to seven subjective parametes

Here, seven additional 'subjective' parameters are included into the model:

-   acousticness
-   danceability (my favorite predictor!)
-   energy
-   instrumentalness
-   liveness
-   speechiness
-   valence

The idea is to find out whether including these is worth the trouble, since assessing these parameters would inevitably involve collecting user ratings. This would be much more complicated than just deriving the 'subjective six' parameters directly from the music by means of suitable algorithms.

#### knn (13 predictors)

This new knn-model will be fitted using the five objective predictors plus the seven additional ones above. Since the knn-model involving the 'objective six' only performed best at high values of k, k-values over a large range will be tried with low resolution in a first step. This should reveal whether high values of k are optimal again. 

```{r}
# train a knn-model with objective and subjective parameters
set.seed(1, sample.kind="Rounding")
train_control <- trainControl(method = "cv", number = n_fold, p = .8)
knn_objective_subjective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms +
                         acousticness + danceability +
                         energy + instrumentalness +
                         liveness + speechiness + 
                         valence, # + artist_name, 
                         method = "knn", 
                       data = train_set, 
                       tuneGrid = data.frame(k = seq(2,502,50)),
                       trControl = train_control)
ggplot(knn_objective_subjective, highlight = TRUE)
# knn_objective_subjective$bestTune$k
```
Indeed, a high number of neigbors improves performance, but the limit leading to 'too many ties' is again reached at k around 500. So in a second step, ks approaching 500 are tested with better resolution.

```{r}
# train a knn-model with objective and subjective parameters
set.seed(1, sample.kind="Rounding")
train_control <- trainControl(method = "cv", number = n_fold, p = .8)
knn_objective_subjective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms +
                         acousticness + danceability +
                         energy + instrumentalness +
                         liveness + speechiness + 
                         valence, # + artist_name, 
                         method = "knn", 
                       data = train_set, 
                       tuneGrid = data.frame(k = seq(460,502,2)),
                       trControl = train_control)
ggplot(knn_objective_subjective, highlight = TRUE)
knn_objective_subjective$bestTune$k
```

The accuracy of this second model on the test set looks as follows:

```{r}
knn_objective_subjective_accuracy <- confusionMatrix(predict(knn_objective_subjective, test_set, type = "raw"), test_set$music_genre)$overall["Accuracy"]
model_results <- bind_rows(model_results,
                          tibble(Method="knn (12 predictors)",
                                     Accuracy = knn_objective_subjective_accuracy))
# kable(model_results, caption = "Model accuracies")
```

#### Random forest (12 predictors)

Finally, a random forest model with all predictors will be trained on the train_set.

```{r}
set.seed(1, sample.kind="Rounding")
control <- trainControl(method = "cv", number = n_fold)
rf_objective_subjective <- train(music_genre ~ popularity + key + loudness + mode + tempo + duration_ms +
                         acousticness + danceability +
                         energy + instrumentalness +
                         liveness + speechiness + 
                         valence, # + artist_name, 
                      method = "rf",
                      ntree = ntree_setting,
                      trControl = control,
                      tuneGrid = grid,
                      nodesize = nodesize_setting,
                      data = train_set)
ggplot(rf_objective_subjective)
rf_objective_subjective$bestTune$mtry
```
Again, best performance is obtained with few predictors, it dips with more being added.

```{r}
rf_objective_accuracy <- confusionMatrix(predict(rf_objective, test_set, type = "raw"), test_set$music_genre)$overall["Accuracy"]
model_results <- bind_rows(model_results,
                          tibble(Method="Random forest (12 predictors)",
                                     Accuracy = rf_objective_accuracy))
# kable(model_results, caption = "Model accuracies")
```

## Results

The performance of the knn model was the same, whether 6 or 13 predictors were included. Both models performed best with a k just below a threshold of 500, where the modelling function raised an error because 'too many ties' affected the voting process. The best k for the six-predictors knn model was `r knn_objective$bestTune$k`, the best k for the full knn-model was `r knn_objective_subjective$bestTune$k`.

The random forest model with six predictors performed best when `r rf_objective$bestTune$mtry` variables were sampled for each tree, and the full model worked best with `r rf_objective_subjective$bestTune$mtry` variables.

The following tabe summarises the overal accuracies of the four models on the test set:

```{r}
kable(model_results, caption = "Model accuracies")

Sys.time()  # when processing ended (in order to find out how long the whole script took to run)
```

Note that both knn-models reached the same accuracy, no matter how many predictors were included. Also, they performed barely above chance levels. Interestingly, the random forest model performed better with fewer predictors.

## Conclusion

The aim of this project was to compare two different modeling approaches to predict music genres based on a fairly large dataset with information about songs. The random forest approache was expected to work reasonably well, while the knn model was predicted to suffer from the 'curse of dimensionality' with the number of predictors included in the models.

These were the main observations:

-   classification based on knn performs terrible with that many predictors, with model performance barely above chance levels
-   normalising predictors could have helped the knn model somewhat, although it is doubtful whether the effort would have had a significant effect on model performance
-   for some reason, the 'too many ties issue' arises at k = 500, I can't mathematically tell why (the limit exist independent of the size of the dataset used; I worked with 10% of the data for most of the development process to limit processing times, but the limit was the same when I used the larger proportions of the entire dataset)
-   the knn-model did not improve when more predictors were added; in fact, it looked and performed exactly the same, no mather whether six or thirteen predictors were included
-   the knn approach may already be suffering from the 'curse of dimensionality' in the leaner version with seven predictors, as the predictor space is complex and distances tend to be large, while differences in distances remaining relatively subtle; an issue which is only exacerbated by adding more predictors
-   model fitting requires very long computation times if all data is included, this imposes limit to crossvalidation and optimisation parameters
-   as expected, the ramdom forest model performed reasonably well
-   however, the random forest model performed better with fewer predictors, which was against expectations

While the knn-approach seems to be unsuitable for this problem, a solution based on an optimised random forest model could be a viable path to solving the automatic classification of songs in a music library. Additional effort would be required to optimse the model further, as several modelling parameters were kept constant here in order to allow for practical computation times.
